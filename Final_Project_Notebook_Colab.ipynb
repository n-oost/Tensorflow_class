{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaca90ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colab-Specific Setup: Mount Drive and Install Dependencies\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Install missing packages (Colab has TF/Keras pre-installed)\n",
    "!pip install -q tensorflow-datasets seaborn scikit-learn\n",
    "\n",
    "print(\"✓ Drive mounted and dependencies installed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1dc31d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment Setup and Imports\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import pathlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "print(\"TensorFlow:\", tf.__version__)\n",
    "print(\"TFDS:\", tfds.__version__)\n",
    "\n",
    "# GPU configuration: enable memory growth if GPU available\n",
    "physical_gpus = tf.config.list_physical_devices('GPU')\n",
    "if physical_gpus:\n",
    "    try:\n",
    "        for gpu in physical_gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(f\"✓ Enabled memory growth on {len(physical_gpus)} GPU(s)\")\n",
    "    except Exception as e:\n",
    "        print(\"GPU config error:\", e)\n",
    "else:\n",
    "    print(\"No GPU detected; running on CPU.\")\n",
    "\n",
    "# Set global seeds for reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "# Mixed precision for faster training on compatible GPUs\n",
    "if physical_gpus:\n",
    "    try:\n",
    "        tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
    "        print(\"✓ Mixed precision enabled.\")\n",
    "    except Exception:\n",
    "        print(\"Mixed precision not enabled.\")\n",
    "\n",
    "# Set artifact directory to Google Drive (persistent across sessions)\n",
    "ARTIFACTS_BASE = pathlib.Path('/content/drive/MyDrive/tf_project_artifacts')\n",
    "ARTIFACTS_BASE.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"✓ Artifacts will be saved to: {ARTIFACTS_BASE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0996b482",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and Inspect Dataset (TFDS - CIFAR-10)\n",
    "(ds_train_full, ds_test), ds_info = tfds.load(\n",
    "    'cifar10',\n",
    "    split=['train', 'test'],\n",
    "    shuffle_files=True,\n",
    "    as_supervised=True,\n",
    "    with_info=True,\n",
    ")\n",
    "\n",
    "# Create validation split from train\n",
    "VAL_FRACTION = 0.1\n",
    "train_size = int((1 - VAL_FRACTION) * ds_info.splits['train'].num_examples)\n",
    "\n",
    "ds_train = ds_train_full.take(train_size)\n",
    "ds_val = ds_train_full.skip(train_size)\n",
    "\n",
    "class_names = ds_info.features['label'].names\n",
    "print(\"Classes:\", class_names)\n",
    "print(\"Train size:\", train_size)\n",
    "print(\"Val size:\", ds_info.splits['train'].num_examples - train_size)\n",
    "print(\"Test size:\", ds_info.splits['test'].num_examples)\n",
    "\n",
    "# Show a small grid of sample images\n",
    "plt.figure(figsize=(6,6))\n",
    "for i, (image, label) in enumerate(ds_train.take(9)):\n",
    "    plt.subplot(3,3,i+1)\n",
    "    plt.imshow(image)\n",
    "    plt.title(class_names[label])\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8610217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image Preprocessing Pipeline (Resize, Normalize, Augment)\n",
    "IMG_SIZE = 224\n",
    "NUM_CLASSES = 10\n",
    "\n",
    "# Basic preprocess: resize to 224x224 and normalize to [0,1]\n",
    "def preprocess_basic(image, label):\n",
    "    image = tf.image.resize(image, (IMG_SIZE, IMG_SIZE))\n",
    "    image = tf.cast(image, tf.float32) / 255.0  # normalize\n",
    "    label = tf.one_hot(label, NUM_CLASSES)\n",
    "    return image, label\n",
    "\n",
    "# Augmentations: random flip, rotation, zoom, contrast\n",
    "# Justification: improves generalization, combats overfitting, and simulates viewpoint variations\n",
    "augment_layers = tf.keras.Sequential([\n",
    "    tf.keras.layers.RandomFlip('horizontal'),\n",
    "    tf.keras.layers.RandomRotation(0.1),\n",
    "    tf.keras.layers.RandomZoom(0.1),\n",
    "    tf.keras.layers.RandomContrast(0.1),\n",
    "])\n",
    "\n",
    "def preprocess_with_augment(image, label):\n",
    "    image = tf.image.resize(image, (IMG_SIZE, IMG_SIZE))\n",
    "    image = tf.cast(image, tf.float32) / 255.0\n",
    "    image = augment_layers(image)\n",
    "    label = tf.one_hot(label, NUM_CLASSES)\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99297ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Baseline CNN Model (from scratch)\n",
    "def build_baseline_cnn(input_shape=(IMG_SIZE, IMG_SIZE, 3), num_classes=NUM_CLASSES):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Conv2D(32, 3, padding='same'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.ReLU(),\n",
    "        tf.keras.layers.Conv2D(32, 3, padding='same'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.ReLU(),\n",
    "        tf.keras.layers.MaxPooling2D(),\n",
    "        tf.keras.layers.Dropout(0.25),\n",
    "\n",
    "        tf.keras.layers.Conv2D(64, 3, padding='same'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.ReLU(),\n",
    "        tf.keras.layers.Conv2D(64, 3, padding='same'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.ReLU(),\n",
    "        tf.keras.layers.MaxPooling2D(),\n",
    "        tf.keras.layers.Dropout(0.25),\n",
    "\n",
    "        tf.keras.layers.Conv2D(128, 3, padding='same'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.ReLU(),\n",
    "        tf.keras.layers.MaxPooling2D(),\n",
    "        tf.keras.layers.Dropout(0.3),\n",
    "\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(256),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.ReLU(),\n",
    "        tf.keras.layers.Dropout(0.4),\n",
    "        tf.keras.layers.Dense(num_classes, dtype='float32'),  # logits\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "baseline_model = build_baseline_cnn()\n",
    "baseline_model.build((None, IMG_SIZE, IMG_SIZE, 3))\n",
    "baseline_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8598d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Transfer Learning Model (MobileNetV2)\n",
    "def build_mobilenetv2_head(input_shape=(IMG_SIZE, IMG_SIZE, 3), num_classes=NUM_CLASSES, fine_tune_at=None):\n",
    "    base = tf.keras.applications.MobileNetV2(\n",
    "        input_shape=input_shape,\n",
    "        include_top=False,\n",
    "        weights='imagenet'\n",
    "    )\n",
    "    base.trainable = False\n",
    "\n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "    x = tf.keras.applications.mobilenet_v2.preprocess_input(inputs)\n",
    "    x = base(x, training=False)\n",
    "    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "    x = tf.keras.layers.Dropout(0.2)(x)\n",
    "    outputs = tf.keras.layers.Dense(num_classes, activation=None, dtype='float32')(x)\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "    # Optional fine-tuning of last N layers\n",
    "    if fine_tune_at is not None and isinstance(fine_tune_at, int):\n",
    "        base.trainable = True\n",
    "        for layer in base.layers[:-fine_tune_at]:\n",
    "            layer.trainable = False\n",
    "    return model\n",
    "\n",
    "transfer_model = build_mobilenetv2_head()\n",
    "transfer_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b05c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile Models (optimizer, loss, metrics)\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "LR = 1e-3\n",
    "\n",
    "baseline_model.compile(\n",
    "    optimizer=optimizers.Adam(learning_rate=LR),\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "transfer_model.compile(\n",
    "    optimizer=optimizers.Adam(learning_rate=LR),\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "print(\"Models compiled.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455da4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare tf.data Pipelines and Batching\n",
    "BATCH_SIZE = 64\n",
    "AUTO = tf.data.AUTOTUNE\n",
    "\n",
    "train_ds = ds_train.map(preprocess_with_augment, num_parallel_calls=AUTO)\n",
    "train_ds = train_ds.cache().shuffle(10_000, seed=SEED).batch(BATCH_SIZE).prefetch(AUTO)\n",
    "\n",
    "val_ds = ds_val.map(preprocess_basic, num_parallel_calls=AUTO)\n",
    "val_ds = val_ds.cache().batch(BATCH_SIZE).prefetch(AUTO)\n",
    "\n",
    "test_ds = ds_test.map(preprocess_basic, num_parallel_calls=AUTO)\n",
    "test_ds = test_ds.cache().batch(BATCH_SIZE).prefetch(AUTO)\n",
    "\n",
    "print(train_ds, val_ds, test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be329511",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Models with Callbacks (early stopping, LR scheduling)\n",
    "ckpt_dir = ARTIFACTS_BASE / 'checkpoints'\n",
    "ckpt_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5, restore_best_weights=True),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3),\n",
    "    tf.keras.callbacks.ModelCheckpoint(filepath=str(ckpt_dir / 'baseline.keras'), save_best_only=True, monitor='val_accuracy'),\n",
    "]\n",
    "\n",
    "start_time = time.time()\n",
    "hist_baseline = baseline_model.fit(train_ds, validation_data=val_ds, epochs=20, callbacks=callbacks)\n",
    "baseline_time = time.time() - start_time\n",
    "\n",
    "callbacks_transfer = [\n",
    "    tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5, restore_best_weights=True),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3),\n",
    "    tf.keras.callbacks.ModelCheckpoint(filepath=str(ckpt_dir / 'transfer.keras'), save_best_only=True, monitor='val_accuracy'),\n",
    "]\n",
    "\n",
    "start_time = time.time()\n",
    "hist_transfer = transfer_model.fit(train_ds, validation_data=val_ds, epochs=20, callbacks=callbacks_transfer)\n",
    "transfer_time = time.time() - start_time\n",
    "\n",
    "print(f\"Baseline total training time: {baseline_time:.1f}s\")\n",
    "print(f\"Transfer total training time: {transfer_time:.1f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43b37e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Training and Validation Curves\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Accuracy\n",
    "axs[0].plot(hist_baseline.history['accuracy'], label='Baseline train')\n",
    "axs[0].plot(hist_baseline.history['val_accuracy'], label='Baseline val')\n",
    "axs[0].plot(hist_transfer.history['accuracy'], label='Transfer train')\n",
    "axs[0].plot(hist_transfer.history['val_accuracy'], label='Transfer val')\n",
    "axs[0].set_title('Accuracy')\n",
    "axs[0].set_xlabel('Epoch')\n",
    "axs[0].set_ylabel('Accuracy')\n",
    "axs[0].legend()\n",
    "\n",
    "# Loss\n",
    "axs[1].plot(hist_baseline.history['loss'], label='Baseline train')\n",
    "axs[1].plot(hist_baseline.history['val_loss'], label='Baseline val')\n",
    "axs[1].plot(hist_transfer.history['loss'], label='Transfer train')\n",
    "axs[1].plot(hist_transfer.history['val_loss'], label='Transfer val')\n",
    "axs[1].set_title('Loss')\n",
    "axs[1].set_xlabel('Epoch')\n",
    "axs[1].set_ylabel('Loss')\n",
    "axs[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eadbb35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Models on Test Set\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import pandas as pd\n",
    "\n",
    "# Evaluate\n",
    "test_loss_base, test_acc_base = baseline_model.evaluate(test_ds, verbose=0)\n",
    "test_loss_trans, test_acc_trans = transfer_model.evaluate(test_ds, verbose=0)\n",
    "print(f\"Baseline - Test Acc: {test_acc_base:.4f}, Loss: {test_loss_base:.4f}\")\n",
    "print(f\"Transfer - Test Acc: {test_acc_trans:.4f}, Loss: {test_loss_trans:.4f}\")\n",
    "\n",
    "# Classification report and confusion matrix\n",
    "# Collect predictions and true labels\n",
    "true_labels = []\n",
    "pred_base = []\n",
    "pred_trans = []\n",
    "for images, labels in test_ds:\n",
    "    y_true = tf.argmax(labels, axis=1).numpy()\n",
    "    true_labels.extend(y_true)\n",
    "    logits_b = baseline_model.predict(images, verbose=0)\n",
    "    logits_t = transfer_model.predict(images, verbose=0)\n",
    "    pred_base.extend(np.argmax(logits_b, axis=1))\n",
    "    pred_trans.extend(np.argmax(logits_t, axis=1))\n",
    "\n",
    "print(\"Baseline classification report:\\n\", classification_report(true_labels, pred_base, target_names=class_names))\n",
    "print(\"Transfer classification report:\\n\", classification_report(true_labels, pred_trans, target_names=class_names))\n",
    "\n",
    "cm_base = confusion_matrix(true_labels, pred_base)\n",
    "cm_trans = confusion_matrix(true_labels, pred_trans)\n",
    "\n",
    "fig, axes = plt.subplots(1,2, figsize=(14,5))\n",
    "sns.heatmap(cm_base, ax=axes[0], cmap='Blues', annot=False)\n",
    "axes[0].set_title('Baseline Confusion Matrix')\n",
    "sns.heatmap(cm_trans, ax=axes[1], cmap='Greens', annot=False)\n",
    "axes[1].set_title('Transfer Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384a1657",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Speed Benchmarking (training time per epoch, throughput)\n",
    "import pandas as pd\n",
    "\n",
    "# Approximate per-epoch time using history logs length\n",
    "epochs_base = len(hist_baseline.history['accuracy'])\n",
    "epochs_trans = len(hist_transfer.history['accuracy'])\n",
    "\n",
    "images_per_epoch = train_size  # approximate number of training samples per epoch\n",
    "throughput_base = images_per_epoch / (baseline_time / max(epochs_base, 1))\n",
    "throughput_trans = images_per_epoch / (transfer_time / max(epochs_trans, 1))\n",
    "\n",
    "summary = pd.DataFrame({\n",
    "    'Model': ['Baseline', 'Transfer'],\n",
    "    'Epochs': [epochs_base, epochs_trans],\n",
    "    'TotalTrainTime(s)': [baseline_time, transfer_time],\n",
    "    'ImagesPerEpoch': [images_per_epoch, images_per_epoch],\n",
    "    'ApproxThroughput(img/s)': [throughput_base, throughput_trans],\n",
    "    'TestAcc': [test_acc_base, test_acc_trans],\n",
    "})\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc49e06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Comparison (accuracy, speed, robustness)\n",
    "val_acc_base = np.array(hist_baseline.history['val_accuracy'])\n",
    "val_acc_trans = np.array(hist_transfer.history['val_accuracy'])\n",
    "\n",
    "comparison = pd.DataFrame({\n",
    "    'Metric': ['TestAcc', 'ValAccStd', 'ApproxThroughput'],\n",
    "    'Baseline': [test_acc_base, float(val_acc_base.std()), float(throughput_base)],\n",
    "    'Transfer': [test_acc_trans, float(val_acc_trans.std()), float(throughput_trans)],\n",
    "})\n",
    "print(comparison)\n",
    "\n",
    "# Simple bar plot for test accuracy\n",
    "plt.figure(figsize=(5,4))\n",
    "plt.bar(['Baseline', 'Transfer'], [test_acc_base, test_acc_trans], color=['steelblue', 'seagreen'])\n",
    "plt.title('Test Accuracy Comparison')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()\n",
    "\n",
    "# Notes: Transfer learning expected to converge faster and achieve higher accuracy\n",
    "# due to pretrained features; baseline may require more epochs and regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5561ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Artifacts (models, histories, plots) to Google Drive\n",
    "import json\n",
    "\n",
    "# Save models\n",
    "baseline_model.save(ARTIFACTS_BASE / 'baseline_savedmodel')\n",
    "transfer_model.save(ARTIFACTS_BASE / 'transfer_savedmodel')\n",
    "\n",
    "# Save histories\n",
    "with open(ARTIFACTS_BASE / 'hist_baseline.json', 'w') as f:\n",
    "    json.dump(hist_baseline.history, f)\n",
    "with open(ARTIFACTS_BASE / 'hist_transfer.json', 'w') as f:\n",
    "    json.dump(hist_transfer.history, f)\n",
    "\n",
    "# Save a figure example\n",
    "fig_path = ARTIFACTS_BASE / 'training_curves.png'\n",
    "fig = plt.figure(figsize=(8,4))\n",
    "plt.plot(hist_baseline.history['val_accuracy'], label='Baseline val_acc')\n",
    "plt.plot(hist_transfer.history['val_accuracy'], label='Transfer val_acc')\n",
    "plt.legend(); plt.title('Validation Accuracy'); plt.xlabel('Epoch'); plt.ylabel('Acc')\n",
    "fig.savefig(fig_path)\n",
    "plt.close(fig)\n",
    "\n",
    "# Export minimal requirements\n",
    "reqs_path = ARTIFACTS_BASE / 'requirements.txt'\n",
    "with open(reqs_path, 'w') as f:\n",
    "    f.write('tensorflow\\n')\n",
    "    f.write('tensorflow-datasets\\n')\n",
    "    f.write('matplotlib\\n')\n",
    "    f.write('seaborn\\n')\n",
    "    f.write('numpy\\n')\n",
    "    f.write('pandas\\n')\n",
    "    f.write('scikit-learn\\n')\n",
    "\n",
    "print(\"✓ Artifacts saved to:\", ARTIFACTS_BASE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915be8af",
   "metadata": {},
   "source": [
    "# Conclusion & Future Work\n",
    "- Summarize key findings: accuracy, speed, and robustness differences between baseline CNN and MobileNetV2 transfer learning on CIFAR-10.\n",
    "- Limitations: dataset size, compute constraints, potential overfitting without careful regularization.\n",
    "- Future work: try fine-tuning deeper layers, experiment with other pretrained models (ResNet50, EfficientNet), add advanced augmentations or regularization, explore Transformers for vision tasks (ViT)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4882b368",
   "metadata": {},
   "source": [
    "# References\n",
    "- TensorFlow and Keras documentation\n",
    "- TensorFlow Datasets (TFDS) CIFAR-10\n",
    "- MobileNetV2 paper and Keras applications\n",
    "- General best practices for image preprocessing and transfer learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777ddb37",
   "metadata": {},
   "source": [
    "# Optional: GAN for CIFAR-10 Truck Class\n",
    "This section implements a DCGAN focused on generating images of the CIFAR-10 `truck` class. It demonstrates:\n",
    "- Class-specific data filtering\n",
    "- Generator (ConvTranspose) and Discriminator (Conv) architectures\n",
    "- Adversarial training loop with non-saturating loss\n",
    "- Periodic sample generation and checkpointing\n",
    "\n",
    "Note: Training a GAN is compute-intensive; for demonstration we use fewer epochs and a subset of data. Increase epochs and dataset size for higher fidelity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10eff1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GAN: Dataset Filtering for Truck Class\n",
    "TRUCK_LABEL = class_names.index('truck') if 'truck' in class_names else 9  # fallback\n",
    "SUBSET_SIZE = 10000  # reduce for quicker demo; set None to use all\n",
    "\n",
    "# Filter train set for trucks only\n",
    "truck_train = ds_train_full.filter(lambda img, lbl: tf.equal(lbl, TRUCK_LABEL))\n",
    "if SUBSET_SIZE:\n",
    "    truck_train = truck_train.take(SUBSET_SIZE)\n",
    "\n",
    "# Basic preprocessing (no augmentation for GAN real images)\n",
    "def gan_preprocess(image, label):\n",
    "    image = tf.image.resize(image, (32, 32))  # keep native size\n",
    "    image = tf.cast(image, tf.float32) / 127.5 - 1.0  # scale to [-1,1]\n",
    "    return image\n",
    "\n",
    "BATCH_GAN = 128\n",
    "AUTO = tf.data.AUTOTUNE\n",
    "\n",
    "gan_ds = truck_train.map(lambda img, lbl: gan_preprocess(img, lbl), num_parallel_calls=AUTO)\n",
    "gan_ds = gan_ds.shuffle(5000, seed=SEED).batch(BATCH_GAN, drop_remainder=True).prefetch(AUTO)\n",
    "print(gan_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c368d727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GAN: Generator and Discriminator\n",
    "LATENT_DIM = 128\n",
    "\n",
    "# Generator: input z -> 32x32x3 using ConvTranspose\n",
    "def build_generator(latent_dim=LATENT_DIM):\n",
    "    inputs = tf.keras.Input(shape=(latent_dim,))\n",
    "    x = tf.keras.layers.Dense(4*4*512, use_bias=False)(inputs)\n",
    "    x = tf.keras.layers.Reshape((4, 4, 512))(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.ReLU()(x)\n",
    "\n",
    "    x = tf.keras.layers.Conv2DTranspose(256, 4, strides=2, padding='same', use_bias=False)(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.ReLU()(x)\n",
    "\n",
    "    x = tf.keras.layers.Conv2DTranspose(128, 4, strides=2, padding='same', use_bias=False)(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.ReLU()(x)\n",
    "\n",
    "    x = tf.keras.layers.Conv2DTranspose(64, 4, strides=2, padding='same', use_bias=False)(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.ReLU()(x)\n",
    "\n",
    "    outputs = tf.keras.layers.Conv2DTranspose(3, 3, activation='tanh', padding='same')(x)\n",
    "    return tf.keras.Model(inputs, outputs, name='generator')\n",
    "\n",
    "# Discriminator: input image -> probability real\n",
    "def build_discriminator():\n",
    "    inputs = tf.keras.Input(shape=(32,32,3))\n",
    "    x = tf.keras.layers.Conv2D(64, 4, strides=2, padding='same')(inputs)\n",
    "    x = tf.keras.layers.LeakyReLU(0.2)(x)\n",
    "    x = tf.keras.layers.Conv2D(128, 4, strides=2, padding='same')(x)\n",
    "    x = tf.keras.layers.LeakyReLU(0.2)(x)\n",
    "    x = tf.keras.layers.Conv2D(256, 4, strides=2, padding='same')(x)\n",
    "    x = tf.keras.layers.LeakyReLU(0.2)(x)\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "    x = tf.keras.layers.Dense(1)(x)  # logits\n",
    "    return tf.keras.Model(inputs, x, name='discriminator')\n",
    "\n",
    "generator = build_generator()\n",
    "discriminator = build_discriminator()\n",
    "\n",
    "generator.summary()\n",
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4cd2ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GAN: Losses, Optimizers, and Training Step\n",
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "G_LR = 2e-4\n",
    "D_LR = 2e-4\n",
    "beta1 = 0.5\n",
    "\n",
    "g_optimizer = tf.keras.optimizers.Adam(G_LR, beta_1=beta1)\n",
    "d_optimizer = tf.keras.optimizers.Adam(D_LR, beta_1=beta1)\n",
    "\n",
    "@tf.function\n",
    "def gan_train_step(real_images):\n",
    "    batch_size = tf.shape(real_images)[0]\n",
    "    random_latent = tf.random.normal((batch_size, LATENT_DIM))\n",
    "\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        fake_images = generator(random_latent, training=True)\n",
    "        real_logits = discriminator(real_images, training=True)\n",
    "        fake_logits = discriminator(fake_images, training=True)\n",
    "\n",
    "        d_loss_real = cross_entropy(tf.ones_like(real_logits), real_logits)\n",
    "        d_loss_fake = cross_entropy(tf.zeros_like(fake_logits), fake_logits)\n",
    "        d_loss = d_loss_real + d_loss_fake\n",
    "\n",
    "        g_loss = cross_entropy(tf.ones_like(fake_logits), fake_logits)\n",
    "\n",
    "    d_grads = tape.gradient(d_loss, discriminator.trainable_variables)\n",
    "    g_grads = tape.gradient(g_loss, generator.trainable_variables)\n",
    "\n",
    "    d_optimizer.apply_gradients(zip(d_grads, discriminator.trainable_variables))\n",
    "    g_optimizer.apply_gradients(zip(g_grads, generator.trainable_variables))\n",
    "\n",
    "    return d_loss, g_loss\n",
    "\n",
    "# Fixed noise for monitoring progress\n",
    "FIXED_LATENT = tf.random.normal((16, LATENT_DIM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146e5683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GAN: Training Loop and Sample Generation\n",
    "EPOCHS_GAN = 10  # Increase for better quality (try 50+ on Colab GPU)\n",
    "SAMPLES_DIR = ARTIFACTS_BASE / 'gan_samples'\n",
    "SAMPLES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for epoch in range(1, EPOCHS_GAN + 1):\n",
    "    start = time.time()\n",
    "    d_losses = []\n",
    "    g_losses = []\n",
    "    for real_batch in gan_ds:\n",
    "        d_loss, g_loss = gan_train_step(real_batch)\n",
    "        d_losses.append(d_loss)\n",
    "        g_losses.append(g_loss)\n",
    "    epoch_d = tf.reduce_mean(d_losses)\n",
    "    epoch_g = tf.reduce_mean(g_losses)\n",
    "\n",
    "    # Generate monitoring samples\n",
    "    generated = generator(FIXED_LATENT, training=False)\n",
    "    generated = (generated + 1.0) / 2.0  # back to [0,1]\n",
    "\n",
    "    fig, axes = plt.subplots(4,4, figsize=(4,4))\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        ax.imshow(generated[i].numpy())\n",
    "        ax.axis('off')\n",
    "    plt.suptitle(f'Epoch {epoch} Samples')\n",
    "    fig.savefig(SAMPLES_DIR / f'epoch_{epoch:03d}.png')\n",
    "    plt.close(fig)\n",
    "\n",
    "    print(f'Epoch {epoch}/{EPOCHS_GAN} D_loss={epoch_d:.4f} G_loss={epoch_g:.4f} time={(time.time()-start):.1f}s')\n",
    "\n",
    "# Save final generator model\n",
    "generator.save(ARTIFACTS_BASE / 'gan_generator_savedmodel')\n",
    "print('✓ GAN training complete. Generator saved to Drive.')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
